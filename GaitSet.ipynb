{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GaitSet Implements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as tordata\n",
    "\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from network.network import SetNet\n",
    "from network.network_layer import *\n",
    "from network.triplet_loss import *\n",
    "from utils.triplet_sampler import *\n",
    "from utils.data_load import *\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = load_OU_ISIR('./data/OU_ISIR/npy/', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_sampler = TripletSampler(train_dataset, [4, 8])\n",
    "\n",
    "train_loader = tordata.DataLoader(dataset=train_dataset, batch_sampler=triplet_sampler, collate_fn=collate_fnn)\n",
    "test_loader = tordata.DataLoader(dataset=test_dataset, batch_size=1, sampler=tordata.sampler.SequentialSampler(test_dataset), collate_fn=collate_fnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = SetNet(128).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = TripletLoss(8, 0.2).to(device)\n",
    "optimizer = optim.Adam([{'params':encoder.parameters()},], lr=3e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.load_state_dict(torch.load('./checkpoint/OU_ISIR_Encoder.ptm'))\n",
    "optimizer.load_state_dict(torch.load('./checkpoint/OU_ISIR_Optimizer.ptm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [100], Elapsed Time [0:01:10.137295], Loss [0.20000015199184418]\n",
      "Step [200], Elapsed Time [0:00:38.019016], Loss [0.2000000923871994]\n",
      "Step [300], Elapsed Time [0:00:32.752270], Loss [0.2000001221895218]\n",
      "Step [400], Elapsed Time [0:00:30.782326], Loss [0.2000001221895218]\n",
      "Step [500], Elapsed Time [0:00:30.766177], Loss [0.2000001221895218]\n",
      "Step [600], Elapsed Time [0:00:30.822944], Loss [0.2000001221895218]\n",
      "Step [700], Elapsed Time [0:00:30.876721], Loss [0.2000001221895218]\n",
      "Step [800], Elapsed Time [0:00:30.777020], Loss [0.2000001221895218]\n",
      "Step [900], Elapsed Time [0:00:30.819044], Loss [0.20000013709068298]\n",
      "Step [1000], Elapsed Time [0:00:30.777726], Loss [0.2000001072883606]\n",
      "Step [1100], Elapsed Time [0:00:30.774788], Loss [0.20000013709068298]\n",
      "Step [1200], Elapsed Time [0:00:30.813654], Loss [0.20000013709068298]\n",
      "Step [1300], Elapsed Time [0:00:30.855591], Loss [0.2000001221895218]\n",
      "Step [1400], Elapsed Time [0:00:30.858564], Loss [0.2000000923871994]\n",
      "Step [1500], Elapsed Time [0:00:30.906408], Loss [0.2000001221895218]\n",
      "Step [1600], Elapsed Time [0:00:30.955277], Loss [0.2000001221895218]\n",
      "Step [1700], Elapsed Time [0:00:31.163845], Loss [0.20000013709068298]\n",
      "Step [1800], Elapsed Time [0:00:31.033597], Loss [0.2000001221895218]\n",
      "Step [1900], Elapsed Time [0:00:31.011104], Loss [0.2000001221895218]\n",
      "Step [2000], Elapsed Time [0:00:33.255154], Loss [0.2000001221895218]\n",
      "Step [2100], Elapsed Time [0:00:30.999158], Loss [0.2000001072883606]\n",
      "Step [2200], Elapsed Time [0:00:31.018136], Loss [0.20000013709068298]\n",
      "Step [2300], Elapsed Time [0:00:31.531365], Loss [0.2000001221895218]\n",
      "Step [2400], Elapsed Time [0:00:31.884139], Loss [0.20000013709068298]\n",
      "Step [2500], Elapsed Time [0:00:31.795890], Loss [0.20000013709068298]\n",
      "Step [2600], Elapsed Time [0:00:31.593233], Loss [0.20000013709068298]\n",
      "Step [2700], Elapsed Time [0:00:31.592198], Loss [0.2000001221895218]\n",
      "Step [2800], Elapsed Time [0:00:31.802713], Loss [0.2000001072883606]\n",
      "Step [2900], Elapsed Time [0:00:31.879598], Loss [0.2000001221895218]\n",
      "Step [3000], Elapsed Time [0:00:32.638791], Loss [0.20000013709068298]\n",
      "Step [3100], Elapsed Time [0:00:32.147088], Loss [0.20000013709068298]\n",
      "Step [3200], Elapsed Time [0:00:32.347657], Loss [0.20000013709068298]\n",
      "Step [3300], Elapsed Time [0:00:32.458466], Loss [0.20000013709068298]\n",
      "Step [3400], Elapsed Time [0:00:32.857761], Loss [0.2000000774860382]\n",
      "Step [3500], Elapsed Time [0:00:31.941438], Loss [0.2000001221895218]\n",
      "Step [3600], Elapsed Time [0:00:31.690120], Loss [0.2000001221895218]\n",
      "Step [3700], Elapsed Time [0:00:32.873546], Loss [0.2000001221895218]\n",
      "Step [3800], Elapsed Time [0:00:32.436595], Loss [0.20000013709068298]\n",
      "Step [3900], Elapsed Time [0:00:32.304980], Loss [0.2000001221895218]\n",
      "Step [4000], Elapsed Time [0:00:33.689690], Loss [0.20000013709068298]\n",
      "Step [4100], Elapsed Time [0:00:33.996525], Loss [0.2000001221895218]\n",
      "Step [4200], Elapsed Time [0:00:32.320467], Loss [0.2000001221895218]\n",
      "Step [4300], Elapsed Time [0:00:32.320237], Loss [0.2000001221895218]\n",
      "Step [4400], Elapsed Time [0:00:32.311338], Loss [0.20000013709068298]\n",
      "Step [4500], Elapsed Time [0:00:32.071058], Loss [0.2000001221895218]\n",
      "Step [4600], Elapsed Time [0:00:32.318323], Loss [0.20000013709068298]\n",
      "Step [4700], Elapsed Time [0:00:32.071478], Loss [0.2000001221895218]\n",
      "Step [4800], Elapsed Time [0:00:31.931476], Loss [0.20000013709068298]\n",
      "Step [4900], Elapsed Time [0:00:32.192073], Loss [0.20000013709068298]\n",
      "Step [5000], Elapsed Time [0:00:32.256246], Loss [0.2000001221895218]\n",
      "Step [5100], Elapsed Time [0:00:32.471736], Loss [0.20000013709068298]\n",
      "Step [5200], Elapsed Time [0:00:32.424245], Loss [0.2000001221895218]\n",
      "Step [5300], Elapsed Time [0:00:32.514031], Loss [0.2000001221895218]\n",
      "Step [5400], Elapsed Time [0:00:32.360796], Loss [0.2000001221895218]\n",
      "Step [5500], Elapsed Time [0:00:32.526003], Loss [0.2000001221895218]\n",
      "Step [5600], Elapsed Time [0:00:32.754180], Loss [0.2000001221895218]\n",
      "Step [5700], Elapsed Time [0:00:32.617041], Loss [0.2000001221895218]\n",
      "Step [5800], Elapsed Time [0:00:32.893689], Loss [0.2000001221895218]\n",
      "Step [5900], Elapsed Time [0:00:32.527675], Loss [0.2000001221895218]\n",
      "Step [6000], Elapsed Time [0:00:34.616528], Loss [0.2000001221895218]\n",
      "Step [6100], Elapsed Time [0:00:32.767292], Loss [0.20000013709068298]\n",
      "Step [6200], Elapsed Time [0:00:32.489105], Loss [0.2000001221895218]\n",
      "Step [6300], Elapsed Time [0:00:31.625111], Loss [0.2000001072883606]\n",
      "Step [6400], Elapsed Time [0:00:32.699029], Loss [0.2000001221895218]\n",
      "Step [6500], Elapsed Time [0:00:31.865637], Loss [0.2000001072883606]\n",
      "Step [6600], Elapsed Time [0:00:32.146386], Loss [0.2000001221895218]\n",
      "Step [6700], Elapsed Time [0:00:32.031477], Loss [0.20000013709068298]\n",
      "Step [6800], Elapsed Time [0:00:31.981425], Loss [0.2000001221895218]\n",
      "Step [6900], Elapsed Time [0:00:31.989021], Loss [0.2000001221895218]\n",
      "Step [7000], Elapsed Time [0:00:31.953459], Loss [0.2000001221895218]\n",
      "Step [7100], Elapsed Time [0:00:31.845256], Loss [0.20000013709068298]\n",
      "Step [7200], Elapsed Time [0:00:32.991151], Loss [0.20000015199184418]\n",
      "Step [7300], Elapsed Time [0:00:31.838023], Loss [0.2000001221895218]\n",
      "Step [7400], Elapsed Time [0:00:32.581470], Loss [0.2000001221895218]\n",
      "Step [7500], Elapsed Time [0:00:32.693907], Loss [0.2000001072883606]\n",
      "Step [7600], Elapsed Time [0:00:32.708592], Loss [0.20000006258487701]\n",
      "Step [7700], Elapsed Time [0:00:33.207581], Loss [0.2000001221895218]\n",
      "Step [7800], Elapsed Time [0:00:33.361646], Loss [0.2000001221895218]\n",
      "Step [7900], Elapsed Time [0:00:33.130061], Loss [0.2000001221895218]\n",
      "Step [8000], Elapsed Time [0:00:34.440299], Loss [0.20000013709068298]\n",
      "Step [8100], Elapsed Time [0:00:33.056403], Loss [0.2000001221895218]\n",
      "Step [8200], Elapsed Time [0:00:31.524757], Loss [0.20000013709068298]\n",
      "Step [8300], Elapsed Time [0:00:32.268240], Loss [0.2000001221895218]\n",
      "Step [8400], Elapsed Time [0:00:32.713328], Loss [0.2000000774860382]\n",
      "Step [8500], Elapsed Time [0:00:33.024134], Loss [0.2000001072883606]\n",
      "Step [8600], Elapsed Time [0:00:32.924978], Loss [0.2000001072883606]\n",
      "Step [8700], Elapsed Time [0:00:32.858179], Loss [0.2000001072883606]\n",
      "Step [8800], Elapsed Time [0:00:32.973894], Loss [0.2000001221895218]\n",
      "Step [8900], Elapsed Time [0:00:32.963978], Loss [0.2000001221895218]\n",
      "Step [9000], Elapsed Time [0:00:33.009598], Loss [0.20000013709068298]\n",
      "Step [9100], Elapsed Time [0:00:32.077277], Loss [0.2000001221895218]\n",
      "Step [9200], Elapsed Time [0:00:31.564653], Loss [0.2000001221895218]\n",
      "Step [9300], Elapsed Time [0:00:31.579599], Loss [0.2000001221895218]\n",
      "Step [9400], Elapsed Time [0:00:31.565644], Loss [0.2000001072883606]\n",
      "Step [9500], Elapsed Time [0:00:31.593547], Loss [0.2000001072883606]\n",
      "Step [9600], Elapsed Time [0:00:31.575592], Loss [0.2000001221895218]\n",
      "Step [9700], Elapsed Time [0:00:31.589581], Loss [0.20000013709068298]\n",
      "Step [9800], Elapsed Time [0:00:31.600575], Loss [0.2000000923871994]\n",
      "Step [9900], Elapsed Time [0:00:31.641393], Loss [0.2000001072883606]\n",
      "Step [10000], Elapsed Time [0:00:32.349527], Loss [0.2000001221895218]\n",
      "Step [10100], Elapsed Time [0:00:34.107851], Loss [0.2000001221895218]\n",
      "Step [10200], Elapsed Time [0:00:31.916578], Loss [0.2000001221895218]\n",
      "Step [10300], Elapsed Time [0:00:31.884576], Loss [0.2000001221895218]\n",
      "Step [10400], Elapsed Time [0:00:32.308208], Loss [0.20000013709068298]\n"
     ]
    }
   ],
   "source": [
    "total_iter = 100000\n",
    "all_losses = []\n",
    "\n",
    "pool = mp.Pool(processes=6)\n",
    "\n",
    "s_time = datetime.now()\n",
    "for i, (seqs, view, label) in enumerate(train_loader):\n",
    "\n",
    "    feature = encoder(seqs)\n",
    "\n",
    "    tmp_label_set = list(train_dataset.set_label)\n",
    "    tmp_label_set.sort()\n",
    "\n",
    "    target_label = [tmp_label_set.index(l) for l in label]\n",
    "    target_label = autograd.Variable(torch.from_numpy(np.array(target_label))).to(device)\n",
    "\n",
    "    triplet_feature = feature.permute(1, 0, 2).contiguous()\n",
    "    triplet_label = target_label.unsqueeze(0).repeat(triplet_feature.size(0), 1)\n",
    "\n",
    "    hard_loss, dist_mean = criterion(triplet_feature, triplet_label)\n",
    "\n",
    "    loss = hard_loss.mean()\n",
    "    \n",
    "    if loss > 1e-9:\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "\n",
    "    if (i+1) % 100 == 0:\n",
    "        print('Step [{}], Elapsed Time [{}], Loss [{}]'.format(i+1, datetime.now() - s_time, loss))\n",
    "        all_losses.append(loss)\n",
    "        s_time = datetime.now()\n",
    "        \n",
    "    if (i+1) == total_iter:\n",
    "        break\n",
    "        \n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder.state_dict(), os.path.join('./checkpoint/','OU_ISIR_Encoder_1.ptm'))\n",
    "torch.save(optimizer.state_dict(), os.path.join('./checkpoint/','OU_ISIR_Optimizer_1.ptm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawLoss(loss_dict):\n",
    "    plt.style.use(['ggplot'])\n",
    "    \n",
    "    for key, value in loss_dict.items():\n",
    "        x = np.arange(len(loss_dict[key]))\n",
    "        plt.plot(x, loss_dict[key], label=key)\n",
    "    \n",
    "    plt.xlabel(\"train step\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "drawLoss({'Loss':all_losses[5000:]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = mp.Pool(processes=6)\n",
    "\n",
    "feature_list = []\n",
    "view_list = []\n",
    "label_list = []\n",
    "\n",
    "s_time = datetime.now()\n",
    "for i, (seqs, view, label) in enumerate(test_loader):\n",
    "\n",
    "    feature = encoder(seqs)\n",
    "    \n",
    "    n, num_bins, _ = feature.size()\n",
    "    feature_list.append(feature.view(n, -1).data.cpu().numpy())\n",
    "    view_list += view\n",
    "    label_list += label\n",
    "    \n",
    "test = np.concatenate(feature_list, 0)\n",
    "        \n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cuda_dist(x, y):\n",
    "    x = torch.from_numpy(x).cuda()\n",
    "    y = torch.from_numpy(y).cuda()\n",
    "    dist = torch.sum(x ** 2, 1).unsqueeze(1) + torch.sum(y ** 2, 1).unsqueeze(\n",
    "        1).transpose(0, 1) - 2 * torch.matmul(x, y.transpose(0, 1))\n",
    "    dist = torch.sqrt(F.relu(dist))\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(feature, view, label):\n",
    "    \n",
    "    label = np.array(label)\n",
    "    view_list = list(set(view))\n",
    "    view_list.sort()\n",
    "    view_num = len(view_list)\n",
    "    sample_num = len(feature)\n",
    "\n",
    "    probe_seq_list = ['00']\n",
    "    gallery_seq_list = ['01']\n",
    "\n",
    "    num_rank = 5\n",
    "    acc = np.zeros([len(probe_seq_list), view_num, view_num, num_rank])\n",
    "    for (p, probe_seq) in enumerate(probe_seq_list):\n",
    "        for gallery_seq in gallery_seq_list:\n",
    "            for (v1, probe_view) in enumerate(view_list):\n",
    "                for (v2, gallery_view) in enumerate(view_list):\n",
    "                    gseq_mask = np.isin(view, [gallery_view])\n",
    "                    gallery_x = feature[gseq_mask, :]\n",
    "                    gallery_y = label[gseq_mask]\n",
    "\n",
    "                    pseq_mask = np.isin(view, [probe_view])\n",
    "                    probe_x = feature[pseq_mask, :]\n",
    "                    probe_y = label[pseq_mask]\n",
    "\n",
    "                    dist = cuda_dist(probe_x, gallery_x)\n",
    "                    idx = dist.sort(1)[1].cpu().numpy()\n",
    "                    acc[p, v1, v2, :] = np.round(\n",
    "                        np.sum(np.cumsum(np.reshape(probe_y, [-1, 1]) == gallery_y[idx[:, 0:num_rank]], 1) > 0,\n",
    "                               0) * 100 / dist.shape[0], 2)\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def de_diag(acc, each_angle=False):\n",
    "    result = np.sum(acc - np.diag(np.diag(acc)), 1) / 10.0\n",
    "    if not each_angle:\n",
    "        result = np.mean(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 14, 14, 5)\n",
      "[[[[100.   100.   100.   100.   100.  ]\n",
      "   [ 53.66  65.85  80.49  82.93  87.8 ]\n",
      "   [ 31.71  41.46  56.1   65.85  75.61]\n",
      "   [ 21.95  26.83  34.15  39.02  43.9 ]\n",
      "   [ 14.63  24.39  34.15  41.46  46.34]\n",
      "   [ 17.07  26.83  39.02  53.66  56.1 ]\n",
      "   [  9.76  21.95  26.83  41.46  43.9 ]\n",
      "   [ 39.02  58.54  63.41  68.29  68.29]\n",
      "   [ 43.9   53.66  63.41  63.41  70.73]\n",
      "   [ 31.71  46.34  58.54  65.85  70.73]\n",
      "   [ 17.07  24.39  36.59  41.46  48.78]\n",
      "   [ 14.63  34.15  43.9   48.78  63.41]\n",
      "   [ 19.51  36.59  39.02  43.9   51.22]\n",
      "   [ 14.63  17.07  26.83  41.46  53.66]]\n",
      "\n",
      "  [[ 53.49  74.42  83.72  83.72  86.05]\n",
      "   [100.   100.   100.   100.   100.  ]\n",
      "   [ 76.74  95.35  95.35  95.35  97.67]\n",
      "   [ 23.26  37.21  41.86  48.84  67.44]\n",
      "   [ 18.6   32.56  44.19  48.84  58.14]\n",
      "   [  6.98  25.58  39.53  48.84  60.47]\n",
      "   [  6.98  25.58  30.23  37.21  48.84]\n",
      "   [ 41.86  48.84  60.47  62.79  69.77]\n",
      "   [ 41.86  60.47  65.12  72.09  79.07]\n",
      "   [ 41.86  51.16  60.47  69.77  81.4 ]\n",
      "   [ 27.91  39.53  53.49  60.47  65.12]\n",
      "   [ 18.6   30.23  37.21  48.84  60.47]\n",
      "   [ 30.23  44.19  48.84  60.47  65.12]\n",
      "   [ 13.95  30.23  44.19  48.84  53.49]]\n",
      "\n",
      "  [[ 40.    53.33  62.22  68.89  68.89]\n",
      "   [ 84.44  93.33  93.33  97.78  97.78]\n",
      "   [100.   100.   100.   100.   100.  ]\n",
      "   [ 66.67  77.78  86.67  88.89  88.89]\n",
      "   [ 35.56  51.11  68.89  73.33  80.  ]\n",
      "   [ 31.11  46.67  53.33  57.78  62.22]\n",
      "   [ 17.78  26.67  33.33  42.22  44.44]\n",
      "   [ 40.    51.11  64.44  64.44  68.89]\n",
      "   [ 48.89  64.44  71.11  75.56  86.67]\n",
      "   [ 57.78  60.    82.22  86.67  88.89]\n",
      "   [ 24.44  48.89  53.33  60.    62.22]\n",
      "   [ 22.22  35.56  51.11  53.33  55.56]\n",
      "   [ 28.89  55.56  60.    71.11  73.33]\n",
      "   [ 11.11  22.22  33.33  51.11  57.78]]\n",
      "\n",
      "  [[ 26.67  37.78  46.67  53.33  55.56]\n",
      "   [ 48.89  62.22  75.56  86.67  86.67]\n",
      "   [ 86.67  93.33  95.56  95.56  97.78]\n",
      "   [100.   100.   100.   100.   100.  ]\n",
      "   [ 71.11  84.44  95.56  95.56  97.78]\n",
      "   [ 48.89  66.67  73.33  82.22  84.44]\n",
      "   [ 17.78  33.33  51.11  57.78  60.  ]\n",
      "   [ 20.    37.78  42.22  53.33  55.56]\n",
      "   [ 44.44  51.11  66.67  66.67  73.33]\n",
      "   [ 37.78  46.67  55.56  62.22  66.67]\n",
      "   [ 28.89  51.11  66.67  73.33  75.56]\n",
      "   [ 20.    42.22  44.44  51.11  60.  ]\n",
      "   [ 31.11  46.67  53.33  60.    64.44]\n",
      "   [ 15.56  28.89  40.    48.89  55.56]]\n",
      "\n",
      "  [[ 15.56  22.22  26.67  37.78  48.89]\n",
      "   [ 13.33  33.33  44.44  53.33  60.  ]\n",
      "   [ 37.78  57.78  73.33  77.78  84.44]\n",
      "   [ 51.11  64.44  73.33  73.33  75.56]\n",
      "   [100.   100.   100.   100.   100.  ]\n",
      "   [ 62.22  75.56  77.78  82.22  84.44]\n",
      "   [ 48.89  60.    68.89  75.56  84.44]\n",
      "   [  8.89  26.67  44.44  51.11  53.33]\n",
      "   [ 20.    33.33  40.    44.44  53.33]\n",
      "   [ 24.44  28.89  42.22  55.56  55.56]\n",
      "   [ 26.67  42.22  46.67  46.67  51.11]\n",
      "   [ 31.11  40.    51.11  55.56  60.  ]\n",
      "   [ 33.33  40.    46.67  51.11  53.33]\n",
      "   [ 15.56  26.67  42.22  51.11  57.78]]\n",
      "\n",
      "  [[ 20.    26.67  31.11  40.    46.67]\n",
      "   [  8.89  31.11  35.56  42.22  48.89]\n",
      "   [ 33.33  48.89  53.33  66.67  71.11]\n",
      "   [ 35.56  53.33  68.89  73.33  75.56]\n",
      "   [ 77.78  86.67  93.33  95.56  95.56]\n",
      "   [100.   100.   100.   100.   100.  ]\n",
      "   [ 68.89  84.44  95.56  97.78  97.78]\n",
      "   [ 11.11  31.11  46.67  53.33  55.56]\n",
      "   [ 24.44  44.44  51.11  53.33  60.  ]\n",
      "   [ 20.    33.33  40.    42.22  55.56]\n",
      "   [ 31.11  44.44  48.89  55.56  57.78]\n",
      "   [ 46.67  60.    68.89  73.33  77.78]\n",
      "   [ 42.22  46.67  57.78  84.44  86.67]\n",
      "   [ 24.44  48.89  66.67  73.33  75.56]]\n",
      "\n",
      "  [[ 11.36  25.    34.09  43.18  47.73]\n",
      "   [ 11.36  18.18  20.45  34.09  36.36]\n",
      "   [ 29.55  45.45  52.27  54.55  59.09]\n",
      "   [ 18.18  40.91  50.    59.09  61.36]\n",
      "   [ 50.    61.36  75.    77.27  84.09]\n",
      "   [ 77.27  86.36  93.18  95.45  95.45]\n",
      "   [100.   100.   100.   100.   100.  ]\n",
      "   [ 11.36  27.27  36.36  43.18  47.73]\n",
      "   [ 18.18  31.82  40.91  47.73  54.55]\n",
      "   [ 13.64  27.27  34.09  38.64  45.45]\n",
      "   [ 22.73  31.82  45.45  45.45  52.27]\n",
      "   [ 34.09  52.27  61.36  70.45  72.73]\n",
      "   [ 36.36  52.27  61.36  72.73  75.  ]\n",
      "   [ 36.36  43.18  52.27  56.82  63.64]]\n",
      "\n",
      "  [[ 32.43  43.24  45.95  54.05  56.76]\n",
      "   [ 29.73  37.84  51.35  56.76  62.16]\n",
      "   [ 29.73  43.24  51.35  54.05  64.86]\n",
      "   [  2.7   27.03  35.14  45.95  45.95]\n",
      "   [ 18.92  32.43  43.24  45.95  51.35]\n",
      "   [ 13.51  24.32  35.14  35.14  45.95]\n",
      "   [ 13.51  27.03  29.73  37.84  40.54]\n",
      "   [100.   100.   100.   100.   100.  ]\n",
      "   [ 54.05  70.27  75.68  86.49  89.19]\n",
      "   [ 37.84  51.35  64.86  70.27  75.68]\n",
      "   [ 24.32  37.84  40.54  45.95  54.05]\n",
      "   [ 27.03  29.73  32.43  48.65  62.16]\n",
      "   [  8.11  24.32  24.32  40.54  45.95]\n",
      "   [ 16.22  24.32  29.73  40.54  40.54]]\n",
      "\n",
      "  [[ 42.22  55.56  57.78  64.44  68.89]\n",
      "   [ 42.22  51.11  60.    75.56  86.67]\n",
      "   [ 44.44  66.67  75.56  80.    82.22]\n",
      "   [ 24.44  35.56  35.56  40.    46.67]\n",
      "   [ 20.    35.56  44.44  51.11  55.56]\n",
      "   [ 17.78  28.89  31.11  42.22  48.89]\n",
      "   [ 22.22  35.56  46.67  51.11  53.33]\n",
      "   [ 44.44  53.33  57.78  66.67  73.33]\n",
      "   [100.   100.   100.   100.   100.  ]\n",
      "   [ 82.22  91.11  93.33  95.56  97.78]\n",
      "   [ 42.22  48.89  62.22  73.33  77.78]\n",
      "   [ 26.67  35.56  46.67  48.89  62.22]\n",
      "   [ 24.44  44.44  55.56  60.    64.44]\n",
      "   [ 24.44  33.33  46.67  55.56  57.78]]\n",
      "\n",
      "  [[ 42.22  44.44  60.    62.22  62.22]\n",
      "   [ 35.56  51.11  64.44  75.56  75.56]\n",
      "   [ 60.    73.33  75.56  77.78  80.  ]\n",
      "   [ 17.78  33.33  37.78  44.44  51.11]\n",
      "   [ 26.67  44.44  46.67  57.78  60.  ]\n",
      "   [ 11.11  24.44  37.78  42.22  48.89]\n",
      "   [ 24.44  26.67  28.89  42.22  53.33]\n",
      "   [ 44.44  53.33  60.    66.67  71.11]\n",
      "   [ 73.33  86.67  95.56  95.56  95.56]\n",
      "   [100.   100.   100.   100.   100.  ]\n",
      "   [ 73.33  84.44  91.11  95.56  97.78]\n",
      "   [ 37.78  62.22  66.67  75.56  77.78]\n",
      "   [ 35.56  46.67  55.56  60.    66.67]\n",
      "   [ 35.56  48.89  48.89  51.11  60.  ]]\n",
      "\n",
      "  [[ 26.67  37.78  48.89  57.78  62.22]\n",
      "   [ 26.67  44.44  55.56  55.56  64.44]\n",
      "   [ 31.11  51.11  71.11  73.33  77.78]\n",
      "   [ 28.89  46.67  60.    64.44  66.67]\n",
      "   [ 28.89  42.22  48.89  55.56  60.  ]\n",
      "   [ 17.78  35.56  51.11  53.33  57.78]\n",
      "   [ 20.    37.78  48.89  55.56  55.56]\n",
      "   [ 33.33  48.89  53.33  60.    66.67]\n",
      "   [ 46.67  62.22  75.56  80.    80.  ]\n",
      "   [ 95.56 100.   100.   100.   100.  ]\n",
      "   [100.   100.   100.   100.   100.  ]\n",
      "   [ 55.56  66.67  80.    91.11  91.11]\n",
      "   [ 53.33  71.11  77.78  82.22  82.22]\n",
      "   [ 28.89  42.22  55.56  64.44  75.56]]\n",
      "\n",
      "  [[ 37.78  44.44  48.89  55.56  57.78]\n",
      "   [ 13.33  24.44  35.56  46.67  51.11]\n",
      "   [ 22.22  42.22  48.89  55.56  60.  ]\n",
      "   [ 13.33  24.44  31.11  42.22  48.89]\n",
      "   [ 20.    28.89  40.    51.11  60.  ]\n",
      "   [ 28.89  42.22  51.11  57.78  62.22]\n",
      "   [ 20.    37.78  48.89  53.33  57.78]\n",
      "   [ 17.78  28.89  35.56  48.89  51.11]\n",
      "   [ 17.78  37.78  48.89  57.78  62.22]\n",
      "   [ 51.11  62.22  77.78  82.22  86.67]\n",
      "   [ 66.67  77.78  84.44  88.89  91.11]\n",
      "   [100.   100.   100.   100.   100.  ]\n",
      "   [ 77.78  88.89 100.   100.   100.  ]\n",
      "   [ 31.11  51.11  60.    84.44  86.67]]\n",
      "\n",
      "  [[ 33.33  37.78  40.    46.67  48.89]\n",
      "   [ 20.    28.89  31.11  42.22  53.33]\n",
      "   [ 22.22  42.22  55.56  60.    60.  ]\n",
      "   [ 15.56  22.22  28.89  33.33  42.22]\n",
      "   [ 28.89  42.22  48.89  55.56  62.22]\n",
      "   [ 31.11  46.67  55.56  60.    75.56]\n",
      "   [ 22.22  46.67  53.33  57.78  68.89]\n",
      "   [ 11.11  31.11  35.56  44.44  46.67]\n",
      "   [ 26.67  44.44  53.33  62.22  66.67]\n",
      "   [ 37.78  60.    66.67  73.33  80.  ]\n",
      "   [ 55.56  66.67  80.    86.67  91.11]\n",
      "   [ 77.78  91.11  95.56 100.   100.  ]\n",
      "   [100.   100.   100.   100.   100.  ]\n",
      "   [ 82.22  91.11  93.33  95.56  95.56]]\n",
      "\n",
      "  [[ 29.55  38.64  40.91  47.73  50.  ]\n",
      "   [ 18.18  31.82  36.36  43.18  45.45]\n",
      "   [ 25.    36.36  40.91  45.45  54.55]\n",
      "   [  4.55  18.18  25.    34.09  38.64]\n",
      "   [ 20.45  43.18  56.82  59.09  63.64]\n",
      "   [ 38.64  47.73  54.55  59.09  70.45]\n",
      "   [ 25.    34.09  43.18  52.27  59.09]\n",
      "   [ 18.18  34.09  38.64  45.45  54.55]\n",
      "   [ 29.55  36.36  47.73  54.55  63.64]\n",
      "   [ 31.82  47.73  50.    59.09  59.09]\n",
      "   [ 36.36  40.91  50.    56.82  68.18]\n",
      "   [ 63.64  75.    79.55  81.82  84.09]\n",
      "   [ 84.09  93.18  95.45 100.   100.  ]\n",
      "   [100.   100.   100.   100.   100.  ]]]]\n"
     ]
    }
   ],
   "source": [
    "acc = evaluation(test, view_list, label_list)\n",
    "print(acc.shape)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 14\n",
      "[54.325      61.15857143 68.93642857 53.77571429 62.47714286 60.91857143\n",
      " 54.85142857 55.89785714 66.78285714 68.82       63.775      66.23642857\n",
      " 66.31357143 59.54142857]\n"
     ]
    }
   ],
   "source": [
    "test_acc = acc[0,:,:,4]\n",
    "result = test_acc - np.diag(np.diag(test_acc))\n",
    "print(len(result), len(result[0]))\n",
    "result = np.mean(result, 0)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Rank-1 (Exclude identical-view cases)===\n",
      "22.975\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    print('===Rank-%d (Exclude identical-view cases)===' % (i + 1))\n",
    "    print(de_diag(acc[0, :, :, i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
